GAP Phylogenomics: Santalaceae  
==============================
B.M. Anderson  
2023–2024  

These notes describe the assembly and analysis of phylogenomic data for samples from the family Santalaceae s.l., focusing on Amphorogynaceae and Viscaceae  

Scripts used are included in the folder `scripts` and include some scripts executed on a supercomputer (NCI Gadi) that contain path information and environmental variables not applicable to most users  

In some cases for commands run on Gadi, the shorthand `/path/to/...` is used and should be replaced with relevant paths to scripts and data being used  
It is assumed scripts are in a folder in the home directory `~/scripts`, or in a directory on the supercomputer `/path/to/scripts`  

# Data
The first stage of the Genomics for Australian Plants (GAP) Australian Angiosperm Tree of Life (AATOL) included the capture and sequencing of 16 samples from the group and from closely-related Santalales (sample numbers starting with 7 or 8)  
Bait-capture libraries were prepared using the Angiosperms353 bait kit (NEBNext Ultra II DNA with MyBaits Angiosperms353 Capture)  
Sequencing was done on an Illumina NovaSeq 6000 in the 150 bp paired-end read format  

For one sample (79894), an initial sequencing run was also done in the 100 bp paired-end read format  

As part of the second stage of AATOL, an additional 48 samples were sequenced (sample numbers starting with 3)  
Bait-capture libraries were prepared using the Angiosperms353 bait kit (NEBNext Ultra II FS DNA Library Prep with MyBaits Angiosperms353 Capture)  
Sequencing was done on an Illumina NovaSeq 6000 in the 150 bp paired-end read format  

To download this data from the Bioplatforms repository (old method, not for ENA), log in there and generate a bulk download (includes a `download.sh` script and a `tmp/` folder with URLs and md5sums)  
On Gadi, copy the script and `tmp/` folder to a location where you want to download the files, then run with your api key (from the Bioplatforms site)  
```s
qsub -v api_key=______ /path/to/scripts/download_seqs.sh
```

An additional 83 samples were sequenced outside Australia (sample numbers starting with M), but four had unclear voucher material and were dropped after sequencing (M29, M43, M70, M87) 
Custom in-house library preparation was done by Arbor Biosciences (Ann Arbor, Michigan, USA) with the A353 bait kit as above   
Sequencing was done on an Illumina NovaSeq 6000 in the 150 bp paired-end read format  

To broaden the dataset, raw sequences for 62 samples were downloaded from the Kew Plant and Fungal Trees of Life (PAFTOL) project (http://sftp.kew.org/pub/treeoflife/)  
These were sourced from A353 datasets generated by PAFTOL and partners, as well as publicly available transcriptomic data including from the OneKP (https://sites.google.com/a/ualberta.ca/onekp/) project  
Additionally, sequences from 2 samples (K58, K59) were downloaded from SRA to broaden the taxonomic sampling  

Given the variable sources of data, read lengths varied from as short as 35 bp, up to 90 bp (OneKP), 125 bp, 151 bp and 301 bp  

In total, there were 16 + 48 + 83 - 4 + 62 + 2 = 207 samples at the outset of the analyses  

Downloads of publicly available sequences with accession numbers were automated with a script on the supercomputer  
```s
# accessions.tab contains sample identifiers and accession numbers, tab separated, one pair per line
qsub -v accessions="accessions.tab" /path/to/scripts/download_ncbi.sh
```

One sample (K30) had multiple sequence accessions, so these were concatenated together into one set of paired read files before proceeding to QC  

# QC
Initial FastQC on the raw data showed non-random base composition (potentially from enzymatic fragmentation; this is a known step for the AATOL samples) in the first few positions for most samples, so these were trimmed from all AATOL samples (first 9 bp) and from the samples sequenced outside Australia for this project (first 10 bp)  
A QC script (`illumina_qc.sh`) was run to remove the problematic bases, as well as remove optical duplicates, trim adapter sequences, and correct sequencing errors, keeping reads at least 50 bp long, using tools from the BBMap package  

For downloaded PAFTOL and SRA samples, some sources formatted the reads so that optical duplicates could not be removed; in addition, differing read lengths and library prep required differing trimming (9, 10 or 12 bases) of the first non-random base pairs  
Because of this variability, no deduplication was done for the PAFTOL/SRA samples  
For SRA samples with >40 M read pairs, seqtk was used to randomly downsample to 40 M (20 M each fastq.gz)  
```s
script="/path/to/scripts/downsample_reads.sh"
for sample in $(cat samples.txt); do
echo -e "${script} $(pwd)/${sample}_R1.fastq.gz $(pwd)/${sample}_R2.fastq.gz" >> calls.txt
done
qsub -l ncpus=16,mem=128GB,walltime=04:00:00,storage=gdata/nm31,wd -v calls_file="calls.txt",cores_per="4",timeout="4000" /path/to/scripts/launch_parallel.sh
```

To automate and parallelise the QC jobs, create a `calls.txt` file for each dataset  
Naming convention for the raw read sources varied, so filenames were adjusted to ensure they started with sample ID followed by an underscore and ended with `R1.fastq.gz` or `R2.fastq.gz`  
Note: for sample 79894 with two read sets, the first read set files were re-named 79894a and the second 79894b prior to QC; they were concatenated into a single set afterward  

For each dataset, a different folder was made to hold QC results: `stage1`, `stage2`, `maya` and `paftol`  
Put sample IDs in a file called `samples.txt` for each dataset (different files for different trim lengths as well if necessary)  
From the `/path/to/qc` folder with reads stored in a `/path/to/raw` folder:
```s
script="/path/to/scripts/illumina_qc.sh"
rawpath="/path/to/raw"
for sample in $(cat samples.txt); do
echo -e "${script} ${rawpath}/${sample}_*R1.fastq.gz ${rawpath}/${sample}_*R2.fastq.gz" >> calls.txt
done
```
(add a "no" argument for those that are not to be deduplicated)  

Launch the jobs using a script to parallelise tasks
```s
qsub -l ncpus=192,mem=768GB,walltime=04:00:00,storage=gdata/nm31,wd -v calls_file="calls.txt",cores_per="16",timeout="4000" /path/to/scripts/launch_parallel.sh
```

Summarise the output  
```s
echo -e "Sample\tReads_raw\tDups_removed\tTrim_corr_removed\tFinal" > summary.txt
for sample in $(cat samples.txt); do
paste <(echo "$sample") <(grep "Reads" "$sample"/readlength_raw.txt | cut -f 2) \
<(grep "Duplicates" "$sample"/"$sample".log | tr -s " " | cut -f 3 -d " " ) \
<(grep "Total Removed" "$sample"/"$sample".log | cut -f 2 | cut -f 1 -d " " | paste -sd+ | bc) \
<(grep "Reads" "$sample"/readlength_qc.txt | cut -f 2) >> summary.txt
done
```

Following QC, three samples (80262, 80678, M02) had fewer than 300 k reads and were dropped from further analysis  
This left 204 samples for assembly  

# Assembly
The HybPiper pipeline v. 2.1.5 (https://github.com/mossmatters/HybPiper) was used to assemble the target loci sequences  

## Target file
Though the Angiosperms353 (A353) bait set has a default target file, we augmented the target file to improve bioinformatic target recovery  

To construct an initial set of targets for the A353 data, the "NewTargets" (https://github.com/chrisjackson-pellicle/NewTargets) approach was used with their `mega353.fasta` file and the script `filter_mega353.py`, with the config file set to add "Santalales" targets; the resulting output was named `targets_Sant.fasta`  

Translate the targets to protein  
```s
python3 ~/scripts/translate.py targets_Sant.fasta
```

To add to those targets, use transcriptomic data from *Viscum album* analysed and published in Schröder et al. 2021 (https://onlinelibrary.wiley.com/doi/10.1111/tpj.15558), which has protein sequences included in supplementary material (Data S3)  

First, turn the Excel sheet into a CSV and convert that to a fasta file using Python3 and Biopython  
(remove the first three rows of the spreadsheet, and keep only columns B and K; remove the encoding characters from Windows at the start of the file: `sed -i '1 s/^.//' tpj15558-sup-0004-datas3.csv`)  
```python
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
with open('tpj15558-sup-0004-datas3.csv', 'r') as infile, open('Viscum_proteins.fasta', 'w') as outfile:
	for line in infile:
		parts = line.strip().split(',')
		outname = parts[0]
		outseq = Seq(parts[1])
		new_record = SeqRecord(outseq, name = outname, id = outname, description = outname)
		SeqIO.write(new_record, outfile, 'fasta')
```
Produces a file with 32,064 protein sequences  

To search these proteins against the A353 loci, build a HMMER (v. 3.3) profile of the A353 loci  
First, make a list of the loci from the descriptions (variable formatting)  
```s
grep ">" targets_Sant_prot.fasta | cut -f 2 -d "-" | cut -f 1 -d " " | cut -f 1 -d "_" | sort -n | uniq | sed -E '/^[a-zA-Z]/d' > loci.txt
```

Create protein fasta files for each A353 locus  
(from a new `temp` directory)  
```python
from Bio import SeqIO
locinames = []
with open('../loci.txt', 'r') as locifile:
	for line in locifile:
		locinames.append(line.strip())

loci = []
with open('../targets_Sant_prot.fasta', 'r') as targetfile:
	targets = SeqIO.parse(targetfile, 'fasta')
	for target in targets:
		for locusname in locinames:
			if locusname in target.description:
				loci.append((locusname, target))
				break

for locusname in locinames:
	with open(str(locusname) + '.fa', 'w') as outfile:
		these_targets = [item[1] for item in loci if item[0] == locusname]
		for target in these_targets:
			SeqIO.write(target, outfile, 'fasta')
```

Build the HMM profiles  
```s
# align sequences with MAFFT
for file in *.fa; do mafft --auto $file > "${file/.fa/.aln}"; done
# build HMM profiles
for file in *.aln; do hmmbuild "${file/.aln/.hmm}" $file; done
# create a single profile of all the A353 loci
cat *.hmm > ../A353.hmm
```

Search the transcript proteins against the HMM profile  
(-E	report sequences <= this E-value threshold)  
Set the evalue strictly (given the smaller database, many good hits have quite small e-values, e.g. 1e-100)  
```s
hmmsearch -E 1e-60 -o hmmer.out A353.hmm Viscum_proteins.fasta
```

Parse the hits and check for duplicate top hits  
```s
python3 ~/scripts/hmmer_parse.py hmmer.out -t 1e-40
cut -f 2 hmmer.out.parsed.filtered | sort | uniq -cd		# none
```
There are 249 protein sequences to add to the target file  

Extract the hit proteins from `Viscum_proteins.fasta`, combining them with the Santalaceae targets in a new file  
```python
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
# read in the list of loci
loci = []
with open('loci.txt', 'r') as infile:
	for line in infile:
		loci.append(line.strip())

# read in the list of proteins to keep
keep_list = []
with open('hmmer.out.parsed.filtered', 'r') as infile:
	for lineno, line in enumerate(infile):
		if lineno != 0:
			keep_list.append(line.strip().split())

# read in the current targets
target_list = []
with open('targets_Sant_prot.fasta', 'r') as infile:
	for entry in SeqIO.parse(infile, 'fasta'):
		target_list.append(entry)

# read in the proteins, adding the desired entries to the target_list
with open('Viscum_proteins.fasta', 'r') as infile:
	for entry in SeqIO.parse(infile, 'fasta'):
		for item in keep_list:
			if entry.name == item[1]:
				outname = 'VISC-' + str(item[0])
				new_record = SeqRecord(entry.seq, name = outname, id = outname, description = outname)
				target_list.append(new_record)
				break

# create the combined targets file
with open('targets_Sant_combined_prot.fasta', 'w') as outfile:
	for locus in loci:
		for target in target_list:
			if target.name.split('-')[-1] == locus:
				SeqIO.write(target, outfile, 'fasta')
```

## Hybpiper
For version 2.1.5 of Hybpiper, recovering intron sequences requires the option `--run_intronerate`  
This will recover both introns and exons, allowing the user to choose which to use in downstream analyses  

Make samples files for each dataset with one sample ID per line:  
`samples_stage1.txt`, `samples_stage2.txt`, `samples_maya.txt`, and `samples_paftol.txt`  
Put the sample IDs for samples with poor vouchers or few reads after QC in a `drop_samples.txt` file  
Link reads for each group of samples, changing paths and samples files appropriately  
```s
# from a folder "hybpiper" with the target and sample files in it
mkdir reads
qcpath="/path/to/qc"
for sample in $(grep -vf drop_samples.txt samples_stage1.txt); do
ln -s "$qcpath"/stage1/$sample/*.fastq.gz reads/
done
```

To make the search settings in HybPiper a little less stringent (allowing more deviation from the target), add an extra argument: `--diamond_sensitivity sensitive` (>40% identity for the diamond search; default: not set = >60% identity)
Now launch the jobs using a script that parallelises them  
```s
cat samples_* | grep -vf drop_samples.txt > samples.txt
qsub -v target_file="targets_Sant_combined_prot.fasta",reads_dir="reads",samples_file="samples.txt",target_type="aa",intronerate="y",add_args="--diamond_sensitivity sensitive" path/to/scripts/hybpiper.sh
```

The jobscript will call HybPiper with the following arguments for each sample:
```s
hybpiper assemble --targetfile_aa targets_Sant_combined_prot.fasta -r {sample}_R1.fastq.gz {sample}_R2.fastq.gz --prefix {sample} --cpu 8 --diamond --merged --run_intronerate --diamond_sensitivity sensitive
```

This took about 2.75 hours to run on Gadi  

Afterwards, to save file space, combine the results from all samples (launch from the directory with the `results` folder)  
```s
qsub -v intronerate="y",delete="y" /path/to/scripts/combine_results.sh
```

While some samples had good recovery, others (with taxonomic bias) did poorly (low recovery and low percentage of reads on target)  
Six samples (377630, 377631, 377633, 377660, 377661 and K12) had fewer than 98 targets with sequences (outliers)  
Add these samples to a `drop_low_samples.txt` file  

Samples had on average 281 targets with a sequence (std dev = 70.4; min = 6; max = 352), but only 195 targets at 50% target length (std dev = 99.8; min = 1; max = 344)  
Percentage of reads on target averaged 5.6% (std dev = 6.2; min = 0.1; max = 23.9)

Paralog warnings were on average 2 and 6 per sample for length and depth, respectively (std dev = 4.0 and 14.8; min = 0 and 0; max = 34 and 130)  
There were a few extremes for paralog warnings: *Mida* (130 and 127 depth warnings), *Nanodea* (75 and 28 depth warnings), *Rhoiacarpos* (45 depth warnings) and *Osyris* (61 depth warnings)  

## Paralogs and coverage
HybPiper will flag and sometimes keep putative paralogs for individual samples/loci based on length of assembled contigs  
Determine which loci had paralog sequences retained  
From the `hybpiper/results/paralog_seqs` folder:
```s
grep ">" *.fasta | grep ".main" | cut -f 1 -d "." | sort | uniq > ../paralog_loci.txt
```
This produces a list of 107 loci  

Let's assess how many taxa had paralogs per locus  
```s
for locus in *.fasta; do
echo $(paste <(echo ${locus/\.fasta/}) <(grep ">" $locus | grep ".main" | wc -l)) >> ../paralog_count.txt
done
```

Sort by the count to see which loci had more taxa with flagged paralogs
```s
# from the hybpiper/results folder
sort -k 2 -k 1 -t " " -n paralog_count.txt
...
6955 10
5919 11
5859 14
6979 14
5347 18
5355 21
5434 28
4951 40
5343 46
```
Most loci had few taxa flagged with paralogs, but there were nine loci with 10 or more taxa flagged as having paralog sequences  
Rather than attempting to recover the phylogenetic signal from these loci, we chose to drop them as potentially unreliable  
Put these loci names in a `paralog_drops.txt` file in the `results` folder  

Check taxon coverage per locus to see if there are any with too few taxa (e.g. <50% of the dataset, or < c.102 samples) to be retained  
```s
# from the hybpiper/results/dna_seqs folder
for locus in *.fasta; do
echo $(paste <(echo ${locus/\.fasta/}) <(grep ">" $locus | wc -l)) >> ../low_taxon_count.txt
done
```

Sort by the count to see which loci had fewer taxa  
```s
# from the hybpiper/results folder
sort -k 2 -k 1 -t " " -n -r low_taxon_count.txt
...
6969 101
6570 100
5064 99
6110 97
7296 95
6995 94
7241 90
5477 89
6449 88
6893 84
6713 84
4744 84
7013 80
5642 72
6407 64
6738 61
6782 60
6865 59
6565 57
6376 54
6946 52
5428 51
6977 42
6886 32
7279 19
6514 8
```
Put the names of these 26 loci (with fewer than 102 samples) in a `low_taxon_drops.txt` file  

Together with the paralog drops, this makes 35 loci dropped of 353, or 318 loci for analysis  

# Phylogenetic analysis (all samples)
To assess overall phylogenetic relationships across Santalales taxa, use protein residue alignments of stitched exon sequences to reduce alignment ambiguity  

Use the HybPiper stitched exons (`hybpiper/results/dna_seqs`) to compile exons from loci and samples of interest in a new directory `hybpiper/filtered_exons`  
```s
# from the hybpiper directory
mkdir filtered_exons && cd filtered_exons
ln -s ../results/dna_seqs/*.fasta .
for droplocus in $(cat ../results/paralog_drops.txt ../results/low_taxon_drops.txt); do
rm "$droplocus".fasta
done
# remove problematic samples  
cat ../drop_samples.txt ../drop_low_samples.txt > temp_drop.txt
for locus in *.fasta; do
python3 /path/to/scripts/remove_fastas.py -f temp_drop.txt "$locus"
done
# rename and replace the links
for file in mod_*; do
mv "$file" "${file/mod_/}"
done
# get rid of any entries beyond the sample ID
for file in *.fasta; do
sed -E -i '/>/s/\s(.+)$//g' "$file"
done
rm temp_drop.txt
```
This resulted in 318 locus files (stitched exons only) with `grep ">" *.fasta | cut -f 2 -d ">" | sort | uniq | wc -l` = 198 taxa  

Make a new folder in the `hybpiper` directory called `phylo_all`, and a `temp_input` folder in that  
For each file of exons, translate to protein, substitute stop codons "*" with "X", align with MAFFT, then use `pal2nal.pl` (Suyama et al. 2006; http://www.bork.embl.de/pal2nal/distribution/pal2nal.v14.tar.gz) to convert the nucleotide files to align with the protein alignment  
```s
# from the temp_input folder
qsub -v fasta_dir="../../filtered_exons" /path/to/scripts/translate_align.sh
```
Took about 20 minutes  

Remove the log files (`rm translate_align*`) before returning to the `phylo_all` directory  
Launch the phylogenetic analysis, cleaning alignments to remove positions with >50% missing data and samples with >75% missing data per locus (but no need for alignment)  
```s
qsub -v align_dir="temp_input",realign="n",clean="y",analysis="a",poly="y",concord="y" /path/to/scripts/align_phylo.sh
```
Took about 13.5 hours  

Calculate summary statistics on the alignments  
```s
# from the folder hybpiper/phylo_all/in_align
python3 ~/scripts/seq_stats.py *.fasta > align_summary.txt
```
This can be imported into a spreadsheet to make summaries by sample  

The filtered and cleaned alignments of 318 loci had a total aligned length of 185 kbp with 18.6% missing data  
Samples had on average 259 loci (std dev = 60; min = 81; max = 318) with an average of 20.9% missing data (std dev = 14.3; min = 0.2; max = 53.3) per locus 

This first run (pre-filtering) can be plotted  
To plot trees, convert output files to appropriate formats  
```s
python3 ~/scripts/concord_to_newick.py -t concord_gcf.cf.tree.nex -o concord_newick
python3 ~/scripts/concord_to_newick.py -t concord_scf.cf.tree.nex -o concord_newick
python3 ~/scripts/astral_parse.py -t astral.tre -f p -o astral
python3 ~/scripts/astral_parse.py -t astral.tre -f q -o astral
# Use Newick Utilities to collapse nodes with ASTRAL polytomy p-values >0.05  
nw_ed astral_poly.tre "i & b > 0.05" o > astral_poly_collapsed.tre
```

Make a file (`samples.tab`) with sample IDs and desired display names (one pair per line, tab separated), and a file (`outgroup.txt`) with sample IDs for the outgroups (K63 and K64 for *Vitis* and *Berberidopsis*, respectively), one per line  

Plot the desired trees interactively using `plot_trees.rmd` and R 4 

Note: when attempting to plot, the outgroup was not monophyletic due to the odd placement of sample M28 Dendromyza nivalis, which appears to be either contaminated/mis-sampled or a misidentification  
Correct the trees by adding it to the outgroup 

## Alignment filtering and run2
To remove potentially misassembled samples or poor alignments, check for erroneous long branches in locus trees
Note: this is potentially problematic in a group with highly divergent taxa (e.g. Balanophoraceae); though they are outliers on long branches, that is expected given previous studies  

Assess branch lengths for the loci using TreeShrink  
```s
singularity exec -H "$(pwd)" /path/to/phylo.sif run_treeshrink.py -t loci.treefile -m per-gene -q 0.10 -O output_ts -o treeshrink
```

Of the 198 samples, `cat treeshrink/output_ts.txt | tr -s '\t' '\n' | sort -n | uniq | wc -l` = 115 were dropped at least once  
Of the 318 loci, `grep -c "^$" treeshrink/output_ts.txt` = 46 had no drops  
Use `awk '{ print NF }' treeshrink/output_ts.txt` to tabulate drops per line; on average there were 3.4 drops per locus with a drop (std dev = 1.9; max = 11)  
Use `cat treeshrink/output_ts.txt | tr '\t' '\n' | sort | uniq -c` to determine how many drops per sample  

Unsurprisingly, the top samples TreeShrink suggested dropping included all four taxa of Balanophoraceae  
The third top sample was M28 Dendromyza nivalis, which may have been misidentified/contaminated and should probably be removed  
Another sample with multiple drops (31) was K35 Octoknema klaineana, which only had 9 genes at 50% target length and is likely not reliable  
Two other samples had >20 drops: 
K61 *Quinchamalium chilense* (relative of *Misodendrum* and *Hachettea*, on long branches sister to Loranthaceae)  
377623 *Choretrum spicatum* ssp. *continentale* (anomalous long branch in the concatenation tree)

Link the exons (pre-alignment) in a new folder to drop undesirable samples
From `phylo_all`
```s
mkdir run2 && cd run2
mkdir exons2 && cd exons2
ln -s ../../../filtered_exons/*.fasta .
```

Let's drop two problematic samples completely: M28 and K35
```s
echo "M28" > temp_drop.txt
echo "K35" >> temp_drop.txt
for locus in *.fasta; do
python3 /path/to/scripts/remove_fastas.py -f temp_drop.txt "$locus"
done
# rename and replace the links
for file in mod_*; do
mv "$file" "${file/mod_/}"
done
```

Next, use the output from TreeShrink to drop specific outlier samples from each locus  
NOTE: skip the Balanophoraceae samples that are expected to be on long branches (K08, K57, K59, K60)  
NOTE: to be fair, also skip the K23 *Hachettea austrocaledonica* (currently Balanophoraceae; 16 drops)
```s
ls ../../in_align/ | cut -f 1 -d "_" > loci.txt		# need to make sure the order is the same as TreeShrink ran
index=1
skiplist="K08 K57 K59 K60 K23"
for locus in $(cat loci.txt); do
remove_list=()
entries=$(sed -n "${index}p; $((index + 1))q" ../../treeshrink/output_ts.txt)
for entry in $entries; do 
if [[ $skiplist =~ $entry ]]; then		# if the entry is in the skiplist
continue
else
remove_list+=($entry)
fi
done
if [ "${#remove_list[@]}" -gt 0 ]; then
remove_line=$(echo ${remove_list[@]} | tr ' ' ',')
python3 /path/to/scripts/remove_fastas.py "$remove_line" "${locus}.fasta"
else
echo "Removed 0 entries from ${locus}.fasta"
fi
((index+=1))
done
# rename and replace links
for file in mod_*; do
mv "$file" "${file/mod_/}"
done
```

Now, re-translate and align
```s
# from the run2 folder
mkdir temp_input && cd temp_input
qsub -v fasta_dir="../exons2" /path/to/scripts/translate_align.sh
```

Run the phylogenetic analysis
```s
# from the run2 folder
qsub -v align_dir="temp_input",realign="n",clean="y",analysis="a",poly="y",concord="y" /path/to/scripts/align_phylo.sh
```
Took about 12 hours  

To plot trees, convert output files to appropriate formats as before  
Plot again, with a new `samples.tab` and `outgroup.txt` file (no longer need to include M28)  

The sample 377623 *Choretrum spicatum* ssp. *continentale* is still on a longer branch in the concatenation tree (though this time closer to another *C. spicatum*); given potential issues with assembly or contamination, it is probably best to drop it from the analysis  

Go back to the exons and drop 377623
```s
for locus in *.fasta; do
python3 /path/to/scripts/remove_fastas.py 377623 "$locus"
done
# rename
for file in mod_*; do
mv "$file" "${file/mod_/}"
done
```

Now, re-translate and align
```s
# from the run2/temp_input folder, after removing the first set of alignments
qsub -v fasta_dir="../exons2" /path/to/scripts/translate_align.sh
```

Run the phylogenetic analysis again
```s
# from the run2 folder, after removing the previous run files
qsub -v align_dir="temp_input",realign="n",clean="y",analysis="a",poly="y",concord="y" /path/to/scripts/align_phylo.sh
```
Took about 11 hours

To plot trees, convert output files to appropriate formats  
```s
python3 ~/scripts/concord_to_newick.py -t concord_gcf.cf.tree.nex -o concord_newick
python3 ~/scripts/concord_to_newick.py -t concord_scf.cf.tree.nex -o concord_newick
python3 ~/scripts/astral_parse.py -t astral.tre -f p -o astral
python3 ~/scripts/astral_parse.py -t astral.tre -f q -o astral
# Use Newick Utilities to collapse nodes with ASTRAL polytomy p-values >0.05  
nw_ed astral_poly.tre "i & b > 0.05" o > astral_poly_collapsed.tre
```

Plot the desired trees interactively using `plot_trees.rmd` and R 4  

If wanting to plot support values (UFB, ASTRAL pp) on the ASTRAL polytomy-collapsed tree, visually compare to the plotted concatenation and ASTRAL trees  

Calculate summary statistics on the alignments  
```s
# from the folder hybpiper/phylo_all/run2/in_align
python3 ~/scripts/seq_stats.py *.fasta > align_summary.txt
```

To generate heat maps of loci and average coverage per locus present, use the order of the tip labels (`tips_astral_poly_collapsed.txt` generated with `plot_trees.rmd`) in a spreadsheet to match with the stats, then create tab separated files (sample ID and stat) as input to create heat maps
```s
Rscript ~/scripts/heatmap.R -f astral_poly_collapsed_loci_count.tab -o astral_poly_collapsed_count -c blues
Rscript ~/scripts/heatmap.R -f astral_poly_collapsed_loci_cover.tab -o astral_poly_collapsed_cover -c greens
```

# Phylogenetic analysis (Amphorogynaceae)
To improve phylogenetic resolution within Amphorogynaceae, use longer alignments including introns for just those taxa  

Make a text file with sample IDs for only Amphorogynaceae (`samples_Amph.txt`)  
Note: exclude 377623 *Choretrum spicatum* ssp. *continentale* given potential assembly/contamination issues  

Use the HybPiper intronerated contigs (`hybpiper/results/supercontig_seqs`) to compile sequences from loci and samples of interest in a new directory `hybpiper/phylo_Amph/temp_input`  
```s
# from the hybpiper/phylo_Amph/temp_input directory
ln -s ../../results/supercontig_seqs/*.fasta .
# remove paralog loci but not low taxon coverage loci (will re-assess for this group)
for droplocus in $(cat ../../results/paralog_drops.txt); do
rm "$droplocus".fasta
done
# remove non-Amphorogynaceae samples
grep -vf ../../samples_Amph.txt ../../samples.txt > temp_drop.txt
for locus in *.fasta; do
python3 /path/to/scripts/remove_fastas.py -f temp_drop.txt "$locus"
done
# rename and replace the links
for file in mod_*; do
mv "$file" "${file/mod_/}"
done
# get rid of any entries beyond the sample ID
for file in *.fasta; do
sed -E -i '/>/s/\s(.+)$//g' "$file"
done
# assess taxon coverage per locus
for locus in *.fasta; do
echo $(paste <(echo ${locus/\.fasta/}) <(grep ">" $locus | wc -l)) >> low_taxon_count.txt
done
sort -k 2 -k 1 -t " " -n -r low_taxon_count.txt
# are there any with <50% or <24 samples?
awk '{if($2 < 24) print $1}' low_taxon_count.txt > temp_rm.txt
wc -l temp_rm.txt
# remove these 13 loci
for locus in $(cat temp_rm.txt); do
rm "$locus".fasta
done
rm *.txt
```
This resulted in 331 locus files (introns + exons) with `grep ">" *.fasta | cut -f 2 -d ">" | sort | uniq | wc -l` = 47 taxa  

Launch the phylogenetic analysis, aligning sequences and cleaning alignments to remove positions with >50% missing data and samples with >75% missing data per locus  
```s
# from the phylo_Amph folder
qsub -v align_dir="temp_input",realign="y",clean="y",analysis="a",poly="y",concord="y" /path/to/scripts/align_phylo.sh
```
Took about 3.5 hours  

The filtered and cleaned alignments of 331 loci had a total aligned length of 441 kbp with 20.4% missing data  
Samples had on average 303 loci (std dev = 43; min = 134; max = 331) with an average of 22.4% missing data (std dev = 17.5; min = 1.5; max = 58.7) per locus 

To plot trees, convert output files to appropriate formats as before  
Make a file (`samples.tab`) with sample IDs and desired display names (one pair per line, tab separated), and a file (`outgroup.txt`) with sample IDs for the "outgroups" (all *Daenikera* and *Amphorogyne*), one per line  

Plot the desired trees interactively using `plot_trees.rmd` and R 4 

## Alignment filtering and run2
To remove potentially misassembled samples or poor alignments, check for erroneous long branches in locus trees
Assess branch lengths for the loci using TreeShrink  
```s
singularity exec -H "$(pwd)" /path/to/phylo.sif run_treeshrink.py -t loci.treefile -m per-gene -q 0.10 -O output_ts -o treeshrink
```

Of the 47 samples, `cat treeshrink/output_ts.txt | tr -s '\t' '\n' | sort -n | uniq | wc -l` = 34 were dropped at least once  
Of the 331 loci, `grep -c "^$" treeshrink/output_ts.txt` = 149 had no drops  
Use `awk '{ print NF }' treeshrink/output_ts.txt` to tabulate drops per line; on average there were 1.3 drops per locus with a drop (std dev = 0.7; max = 8)  
Use `cat treeshrink/output_ts.txt | tr '\t' '\n' | sort | uniq -c` to determine how many drops per sample  

The top dropped sample was K16 *Daenikera* sp. (55 drops; more divergent from the remainder, which might partly explain it), then 377640 *Leptomeria axillaris* (28 drops)  

Next, use the output from TreeShrink to drop specific outlier samples from each locus  
```s
# from the phylo_Amph folder
mkdir run2 && cd run2
mkdir temp_input && cd temp_input
cp ../../temp_input/*.fasta .
ls ../../in_align/ | cut -f 1 -d "_" > loci.txt		# need to make sure the order is the same as TreeShrink ran
index=1
for locus in $(cat loci.txt); do
remove_list=()
entries=$(sed -n "${index}p; $((index + 1))q" ../../treeshrink/output_ts.txt)
for entry in $entries; do 
remove_list+=($entry)
done
if [ "${#remove_list[@]}" -gt 0 ]; then
remove_line=$(echo ${remove_list[@]} | tr ' ' ',')
python3 /path/to/scripts/remove_fastas.py "$remove_line" "${locus}.fasta"
else
echo "Removed 0 entries from ${locus}.fasta"
fi
((index+=1))
done
# rename modified files
for file in mod_*; do
mv "$file" "${file/mod_/}"
done
```

Run the phylogenetic analysis again, aligning sequences and cleaning alignments to remove positions with >50% missing data and samples with >75% missing data per locus  
```s
# from the phylo_Amph/run2 folder
qsub -v align_dir="temp_input",realign="y",clean="y",analysis="a",poly="y",concord="y" /path/to/scripts/align_phylo.sh
```
Took about 3.2 hours  

# Phylogenetic analysis (*Korthalsella*)
To similarly improve phylogenetic resolution within *Korthalsella* and its sister clades, use longer alignments including introns for just those taxa  

Make a text file with sample IDs for only the *Korthalsella* clade and its sister (`samples_Kor.txt`)  

Use the HybPiper intronerated contigs (`hybpiper/results/supercontig_seqs`) to compile sequences from loci and samples of interest in a new directory `hybpiper/phylo_Kor/temp_input`  
```s
# from the hybpiper/phylo_Kor/temp_input directory
ln -s ../../results/supercontig_seqs/*.fasta .
# remove paralog loci but not low taxon coverage loci (will re-assess for this group)
for droplocus in $(cat ../../results/paralog_drops.txt); do
rm "$droplocus".fasta
done
# remove non-Korthalsella samples
grep -vf ../../samples_Kor.txt ../../samples.txt > temp_drop.txt
for locus in *.fasta; do
python3 /path/to/scripts/remove_fastas.py -f temp_drop.txt "$locus"
done
# rename and replace the links
for file in mod_*; do
mv "$file" "${file/mod_/}"
done
# get rid of any entries beyond the sample ID
for file in *.fasta; do
sed -E -i '/>/s/\s(.+)$//g' "$file"
done
# assess taxon coverage per locus
for locus in *.fasta; do
echo $(paste <(echo ${locus/\.fasta/}) <(grep ">" $locus | wc -l)) >> low_taxon_count.txt
done
sort -k 2 -k 1 -t " " -n -r low_taxon_count.txt
# are there any with <50% or <19 samples?
awk '{if($2 < 19) print $1}' low_taxon_count.txt > temp_rm.txt
wc -l temp_rm.txt
# remove these 73 loci
for locus in $(cat temp_rm.txt); do
rm "$locus".fasta
done
rm *.txt
```
This resulted in 271 locus files (introns + exons) with `grep ">" *.fasta | cut -f 2 -d ">" | sort | uniq | wc -l` = 37 taxa  

Launch the phylogenetic analysis, aligning sequences and cleaning alignments to remove positions with >50% missing data and samples with >75% missing data per locus  
```s
# from the phylo_Kor folder
qsub -v align_dir="temp_input",realign="y",clean="y",analysis="a",poly="y",concord="y" /path/to/scripts/align_phylo.sh
```
Took about 1.5 hours 

The filtered and cleaned alignments of 271 loci had a total aligned length of 272 kbp with 21.5% missing data  
Samples had on average 222 loci (std dev = 50; min = 80; max = 268) with an average of 24.4% missing data (std dev = 15.5; min = 3.6; max = 59.3) per locus 

To plot trees, convert output files to appropriate formats as before  
Make a file (`samples.tab`) with sample IDs and desired display names (one pair per line, tab separated), and a file (`outgroup.txt`) with sample IDs for the "outgroups" (all *Dendrophthera* and *Phoradendron*), one per line  

Plot the desired trees interactively using `plot_trees.rmd` and R 4 

## Alignment filtering and run2
To remove potentially misassembled samples or poor alignments, check for erroneous long branches in locus trees
Assess branch lengths for the loci using TreeShrink  
```s
singularity exec -H "$(pwd)" /path/to/phylo.sif run_treeshrink.py -t loci.treefile -m per-gene -q 0.10 -O output_ts -o treeshrink
```

Of the 37 samples, `cat treeshrink/output_ts.txt | tr -s '\t' '\n' | sort -n | uniq | wc -l` = 31 were dropped at least once  
Of the 271 loci, `grep -c "^$" treeshrink/output_ts.txt` = 156 had no drops  
Use `awk '{ print NF }' treeshrink/output_ts.txt` to tabulate drops per line; on average there were 1.3 drops per locus with a drop (std dev = 0.9; max = 7)  
Use `cat treeshrink/output_ts.txt | tr '\t' '\n' | sort | uniq -c` to determine how many drops per sample  

The top dropped sample was K40 *Phoradendron leucarpum* (49 drops; was on a long branch in the concatenation tree), potentially indicating some assembly issues; the next was 377634 *Korthalsella leucothrix* (20 drops)  

Use the output from TreeShrink to drop specific outlier samples from each locus  
```s
# from the phylo_Kor folder
mkdir run2 && cd run2
mkdir temp_input && cd temp_input
cp ../../temp_input/*.fasta .
ls ../../in_align/ | cut -f 1 -d "_" > loci.txt		# need to make sure the order is the same as TreeShrink ran
index=1
for locus in $(cat loci.txt); do
remove_list=()
entries=$(sed -n "${index}p; $((index + 1))q" ../../treeshrink/output_ts.txt)
for entry in $entries; do 
remove_list+=($entry)
done
if [ "${#remove_list[@]}" -gt 0 ]; then
remove_line=$(echo ${remove_list[@]} | tr ' ' ',')
python3 /path/to/scripts/remove_fastas.py "$remove_line" "${locus}.fasta"
else
echo "Removed 0 entries from ${locus}.fasta"
fi
((index+=1))
done
# rename modified files
for file in mod_*; do
mv "$file" "${file/mod_/}"
done
```

Run the phylogenetic analysis again, aligning sequences and cleaning alignments to remove positions with >50% missing data and samples with >75% missing data per locus  
```s
# from the phylo_Kor/run2 folder
qsub -v align_dir="temp_input",realign="y",clean="y",analysis="a",poly="y",concord="y" /path/to/scripts/align_phylo.sh
```
